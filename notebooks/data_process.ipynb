{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "import time\r\n",
    "import json \r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from tqdm import tqdm\r\n",
    "from tweepy import Cursor, TweepError, OAuthHandler, API\r\n",
    "from typing import List, Dict\r\n",
    "from collections import defaultdict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# connect to Twitter API\r\n",
    "consumer_key = 'CONSUMER_KEY'\r\n",
    "secret_key = 'SECRET_KEY'\r\n",
    "access_token = 'ACCESS_TOKEN'\r\n",
    "access_token_secret = 'ACCESS_TOKEN_SECRET'\r\n",
    "auth = OAuthHandler(consumer_key, secret_key)\r\n",
    "auth.set_access_token(access_token, access_token_secret)\r\n",
    "\r\n",
    "api = API(auth, wait_on_rate_limit=True,\r\n",
    "          wait_on_rate_limit_notify=True, compression=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def read_lines(path: str):\r\n",
    "    file = open(path, 'r')\r\n",
    "    content = file.read().splitlines()\r\n",
    "    file.close()\r\n",
    "    if not content:\r\n",
    "        raise Exception(f'file: {path} is empty.')\r\n",
    "    return content"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load seed\r\n",
    "screen_names = read_lines('seed_usernames.txt')\r\n",
    "df = pd.read_csv('seed_ids.csv')\r\n",
    "seed = df['id']\r\n",
    "if len(seed) < len(screen_names):\r\n",
    "    missing = set(df['screen_name']) ^ set(screen_names)\r\n",
    "    print(f'Found {len(missing)} missing user id(s).')\r\n",
    "    new_users = ''\r\n",
    "    for m in tqdm(missing, total=len(missing), ncols=80):\r\n",
    "        print(f'Searching for {m}...')\r\n",
    "        user = api.get_user(m)\r\n",
    "        new_users += f'{m},{user.id_str}\\n'\r\n",
    "    with open('seed_ids.csv', 'a') as f:\r\n",
    "        f.write(new_users)\r\n",
    "    print('Done.')\r\n",
    "    df = pd.read_csv('seed_ids.csv')\r\n",
    "    seed = df['id']\r\n",
    "seed = list(seed)\r\n",
    "print(f'Loaded {len(seed)} seed users')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def dataset_info(path='tmp/dataset.csv'):\r\n",
    "    if os.path.exists(path):\r\n",
    "        nodes = set()\r\n",
    "        df = pd.read_csv(path)\r\n",
    "        nodes = set(df.source.values)\r\n",
    "        nodes.update(df.target.values)\r\n",
    "        print(f'Number of nodes: {len(nodes)}')\r\n",
    "        print(f'Number of edges: {len(df)}')\r\n",
    "    else:\r\n",
    "        print('No data found.')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def friendships_already_saved(user_id:int, friendship:str, path:str) -> bool:\r\n",
    "    if not os.path.exists(path):\r\n",
    "        return False\r\n",
    "    df = pd.read_csv(path)\r\n",
    "    column = df.source if friendship == 'followees' else df.target\r\n",
    "    return user_id in column.values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def save_friendships(user_id:int, ids:List[int], friendship:str, path:str) -> None:\r\n",
    "    reduced = ''\r\n",
    "    if friendship == 'followees':\r\n",
    "        for i in ids:\r\n",
    "            reduced += f'{user_id},{i}\\n'\r\n",
    "    elif friendship == 'followers':\r\n",
    "        for i in ids:\r\n",
    "            reduced += f'{i},{user_id}\\n'\r\n",
    "    else:\r\n",
    "        return\r\n",
    "    file_exists = os.path.exists(path)\r\n",
    "    with open(path, 'a') as f:\r\n",
    "        if not file_exists:\r\n",
    "            f.write('source,target\\n')\r\n",
    "        f.write(reduced)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def retrieve_friendships(user_id:int, friendship:str, limit:int, save_path:str):\r\n",
    "    try:\r\n",
    "        f = api.friends_ids if friendship == 'followees' else api.followers_ids\r\n",
    "        ids = list(Cursor(f, id=user_id).items(limit))\r\n",
    "        if not friendships_already_saved(user_id, friendship, save_path):\r\n",
    "            save_friendships(user_id, ids, friendship, path=save_path)\r\n",
    "    except TweepError as e:\r\n",
    "        pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def retrieve_friendships_from_file(file_path:str, save_path:str, column:str, friendship:str, limit:int) -> None:\r\n",
    "    if not os.path.exists(file_path):\r\n",
    "        print('No file found.')\r\n",
    "        return\r\n",
    "    df = pd.read_csv(file_path)\r\n",
    "    user_ids = df[column].unique()\r\n",
    "    idx = np.random.randint(len(user_ids), size=limit)\r\n",
    "    user_ids = user_ids[idx]\r\n",
    "    for user_id in tqdm(user_ids, total=len(user_ids), desc=f'{os.path.basename(file_path)}[{column}] {friendship}'):\r\n",
    "        retrieve_friendships(user_id, friendship, limit, save_path)\r\n",
    "        time.sleep(20)\r\n",
    "    print('\\nDone!')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get seed followees\r\n",
    "for s in tqdm(seed, total=len(seed), desc='Followees'):\r\n",
    "    retrieve_friendships(s, 'followees', 100, 'tmp/followees/0.csv')\r\n",
    "    time.sleep(20)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get depth 1 followees\r\n",
    "source = 'tmp/followees/0.csv'\r\n",
    "target = 'tmp/followees/1.csv'\r\n",
    "retrieve_friendships_from_file(source, target, column='target', friendship='followees', limit=100)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get depth 2 followees\r\n",
    "source = 'tmp/followees/1.csv'\r\n",
    "target = 'tmp/followees/2.csv'\r\n",
    "retrieve_friendships_from_file(source, target, column='target', friendship='followees', limit=100)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get seed followers\r\n",
    "for s in tqdm(seed, total=len(seed), desc='Followers'):\r\n",
    "    retrieve_friendships(s, 'followers', 100, 'tmp/followers/0.csv')\r\n",
    "    time.sleep(20)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get depth 1 followers\r\n",
    "source = 'tmp/followers/0.csv'\r\n",
    "target = 'tmp/followers/1.csv'\r\n",
    "retrieve_friendships_from_file(source, target, column='source', friendship='followers', limit=100)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get depth 2 followers\r\n",
    "source = 'tmp/followers/1.csv'\r\n",
    "target = 'tmp/followers/2.csv'\r\n",
    "retrieve_friendships_from_file(source, target, column='source', friendship='followers', limit=100)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df0 = pd.read_csv('tmp/followees/0.csv')\r\n",
    "df1 = pd.read_csv('tmp/followees/1.csv')\r\n",
    "df2 = pd.read_csv('tmp/followees/2.csv')\r\n",
    "df = pd.concat([df0, df1, df2])\r\n",
    "df.to_csv('tmp/followees/followees.csv', index=False)\r\n",
    "len(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df0 = pd.read_csv('tmp/followers/0.csv')\r\n",
    "df1 = pd.read_csv('tmp/followers/1.csv')\r\n",
    "df2 = pd.read_csv('tmp/followers/2.csv')\r\n",
    "df = pd.concat([df0, df1, df2])\r\n",
    "df.to_csv('tmp/followers/followers.csv', index=False)\r\n",
    "len(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "followees_df = pd.read_csv('tmp/followees/followees.csv')\r\n",
    "followers_df = pd.read_csv('tmp/followers/followers.csv')\r\n",
    "df = pd.concat([followees_df, followers_df])\r\n",
    "df.to_csv('tmp/dataset.csv', index=False)\r\n",
    "len(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset_info('tmp/dataset.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def retrieve_user(user_id: int) -> Dict:\r\n",
    "    try:\r\n",
    "        user = api.get_user(user_id)\r\n",
    "        entities = user.entities\r\n",
    "        urls = []\r\n",
    "        try:\r\n",
    "            for e in entities.values():\r\n",
    "                for url in e['urls']:\r\n",
    "                    urls.append(url['display_url'])\r\n",
    "        except Exception:\r\n",
    "           pass\r\n",
    "        user_data = {\r\n",
    "            'id': user.id,\r\n",
    "            'name': user.name,\r\n",
    "            'screen_name': user.screen_name,\r\n",
    "            'location': user.location,\r\n",
    "            'description': user.description,\r\n",
    "            'urls': urls,\r\n",
    "            'protected': user.protected,\r\n",
    "            'verified': user.verified,\r\n",
    "            'followers_count': user.followers_count,\r\n",
    "            'friends_count': user.friends_count,\r\n",
    "            'listed_count': user.listed_count,\r\n",
    "            'statuses_count': user.statuses_count,\r\n",
    "            'default_profile': user.default_profile,\r\n",
    "            'default_profile_image': user.default_profile_image\r\n",
    "        }\r\n",
    "        return user_data\r\n",
    "    except TweepError as e:\r\n",
    "        print(e.reason)\r\n",
    "    return None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_unique_users(path='tmp/dataset.csv'):\r\n",
    "    df = pd.read_csv(path)\r\n",
    "    users = set(df.source.unique())\r\n",
    "    users.update(df.target.unique())\r\n",
    "    return users"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_missing_users(cache: Dict, dataset_path='tmp/dataset.csv'):\r\n",
    "    if not os.path.exists(dataset_path):\r\n",
    "        print('No dataset found.')\r\n",
    "        return\r\n",
    "    users = set()\r\n",
    "    df = pd.read_csv(dataset_path)\r\n",
    "    users.update(df.source.values)\r\n",
    "    users.update(df.target.values)\r\n",
    "    return users - set([int(key) for key in cache.keys()])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def update_user_cache(user_cache: Dict, file_path='tmp/users.json'):\r\n",
    "    with open(file_path,'r+') as f:\r\n",
    "        cache = json.load(f)\r\n",
    "        cache.update(user_cache)\r\n",
    "        f.seek(0)\r\n",
    "        json.dump(cache, f, indent=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# retrieve user data\r\n",
    "cache_path = 'tmp/users.json'\r\n",
    "user_cache = {}\r\n",
    "if os.path.exists(cache_path):\r\n",
    "    with open(cache_path, 'r') as f:\r\n",
    "        user_cache = json.load(f)\r\n",
    "\r\n",
    "users = get_missing_users(user_cache, 'tmp/dataset.csv')\r\n",
    "not_found = set()\r\n",
    "\r\n",
    "del user_cache\r\n",
    "user_cache = {}\r\n",
    "\r\n",
    "if not users:\r\n",
    "    print('No users to retrieve.')\r\n",
    "else:\r\n",
    "    for i, user_id in tqdm(enumerate(users, start=1), total=len(users), ncols=80):\r\n",
    "        user = retrieve_user(user_id)\r\n",
    "        if user:\r\n",
    "            user_cache[str(user_id)] = user\r\n",
    "        else:\r\n",
    "            not_found.add(user_id)\r\n",
    "        if i % 10 == 0:\r\n",
    "            update_user_cache(user_cache, cache_path)\r\n",
    "    update_user_cache(user_cache, cache_path)\r\n",
    "    print('Done.')"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# filter out users with missing data\r\n",
    "df = pd.read_csv('tmp/dataset.csv')\r\n",
    "for user_id in not_found:\r\n",
    "    indexNames = df[(df.source == user_id) | (df.target == user_id)].index\r\n",
    "    df.drop(indexNames, inplace=True)\r\n",
    "df.to_csv('tmp/dataset.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# set user indices\r\n",
    "users = get_unique_users(path='tmp/dataset_demo_5000.csv') # here\r\n",
    "\r\n",
    "idx = {}\r\n",
    "for i, user_id in tqdm(enumerate(users), total=len(users)):\r\n",
    "    idx[str(user_id)] = i\r\n",
    "\r\n",
    "with open('tmp/user_idx_demo_5000.json', 'w') as f:\r\n",
    "    json.dump(idx, f, indent=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# build edge list\r\n",
    "with open('tmp/user_idx_demo_5000.json', 'r') as f:\r\n",
    "    idx = json.load(f)\r\n",
    "\r\n",
    "edge_list = pd.read_csv('tmp/dataset_demo_5000.csv').to_numpy()\r\n",
    "for edge in tqdm(edge_list, total=len(edge_list)):\r\n",
    "    for i, user_id in enumerate(edge):\r\n",
    "        edge[i] = idx[str(user_id)]\r\n",
    "\r\n",
    "df = pd.DataFrame(edge_list, columns=['source', 'target'])\r\n",
    "df.to_csv('data/edges_demo_5000.csv', index=False)\r\n",
    "print('\\nDone!')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tldextract\r\n",
    "from ttp import ttp\r\n",
    "import tldextract\r\n",
    "from tldextract.tldextract import ExtractResult\r\n",
    "\r\n",
    "parser = ttp.Parser()"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load research fields\r\n",
    "global research_fields\r\n",
    "research_fields = read_lines('research_fields.txt')\r\n",
    "research_fields[:5]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# url verifications\r\n",
    "def is_google_research_url(url: ExtractResult) -> bool:\r\n",
    "    return url.subdomain in ['sites', 'scholar'] and url.domain == 'google'\r\n",
    "\r\n",
    "def is_linkedin_url(url: ExtractResult) -> bool:\r\n",
    "    return url.domain == 'linkedin'\r\n",
    "\r\n",
    "def is_github_url(url: ExtractResult) -> bool:\r\n",
    "    return url.domain == 'github'\r\n",
    "\r\n",
    "def url_has_edu(url: ExtractResult) -> bool:\r\n",
    "    return url.suffix.startswith('edu')\r\n",
    "\r\n",
    "def url_is_academic(url: ExtractResult) -> bool:\r\n",
    "    return url.suffix.startswith('ac')\r\n",
    "\r\n",
    "def url_has_tilde(url: str) -> bool:\r\n",
    "    return '~' in url\r\n",
    "\r\n",
    "def verify_urls(urls: List[str]) -> List[int]:\r\n",
    "    res = np.zeros(7, dtype=np.int8)\r\n",
    "    for url in urls:\r\n",
    "        tmp = np.zeros(7, dtype=np.int8)\r\n",
    "        ext = tldextract.extract(url)\r\n",
    "        tmp[0] = 1\r\n",
    "        tmp[1] = is_google_research_url(ext)\r\n",
    "        tmp[2] = is_linkedin_url(ext)\r\n",
    "        tmp[3] = is_github_url(ext)\r\n",
    "        tmp[4] = url_has_edu(ext)\r\n",
    "        tmp[5] = url_is_academic(ext)\r\n",
    "        tmp[6] = url_has_tilde(url)\r\n",
    "        res = res | tmp\r\n",
    "    return list(res), any(res[[1, 4, 5]])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_description_research_fields_count(desc: str) -> bool:\r\n",
    "    desc = desc.lower()\r\n",
    "    return sum(field in desc or ''.join(field.split()) in desc for field in research_fields)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# normalization\r\n",
    "def min_max_norm(df:pd.DataFrame, column:str):\r\n",
    "    df_min_max_scaled = df.copy()\r\n",
    "    df_min_max_scaled[column] = (df_min_max_scaled[column] - df_min_max_scaled[column].min()) / (df_min_max_scaled[column].max() - df_min_max_scaled[column].min())    \r\n",
    "    return df_min_max_scaled\r\n",
    "\r\n",
    "def z_score_norm(df:pd.DataFrame, column:str):\r\n",
    "    df_z_scaled = df.copy()\r\n",
    "    df_z_scaled[column] = (df_z_scaled[column] - df_z_scaled[column].mean()) / df_z_scaled[column].std()\r\n",
    "    return df_z_scaled\r\n",
    "\r\n",
    "def std_norm(df:pd.DataFrame, column:str):\r\n",
    "    df_std_scaled = df.copy()\r\n",
    "    df_std_scaled[column] = df_std_scaled[column] / df_std_scaled[column].std()\r\n",
    "    return df_std_scaled"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.read_csv('data/features_demo_5000.csv')\r\n",
    "\r\n",
    "df = min_max_norm(df, 'followers_count')\r\n",
    "df = min_max_norm(df, 'friends_count')\r\n",
    "df = min_max_norm(df, 'listed_count')\r\n",
    "df = min_max_norm(df, 'statuses_count')\r\n",
    "\r\n",
    "df = min_max_norm(df, 'ratio_followers_friends')\r\n",
    "df = min_max_norm(df, 'description_research_fields_count')\r\n",
    "df = min_max_norm(df, 'description_mention_count')\r\n",
    "df = min_max_norm(df, 'description_hashtag_count')\r\n",
    "df = min_max_norm(df, 'description_url_count')\r\n",
    "# df.drop(['ratio_followers_friends'], axis = 1, inplace=True)\r\n",
    "\r\n",
    "\r\n",
    "df.to_csv('data/features_demo_5000.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def generate_feature_vector(user: Dict) -> List :\r\n",
    "    features = []\r\n",
    "    features.append(int(user['protected']))\r\n",
    "    features.append(int(user['verified']))\r\n",
    "    followers = user['followers_count']\r\n",
    "    friends = user['friends_count']\r\n",
    "    features.append(followers)\r\n",
    "    features.append(friends)\r\n",
    "    features.append(followers/friends if friends else 0)\r\n",
    "    features.append(user['listed_count'])\r\n",
    "    features.append(user['statuses_count'])\r\n",
    "    features.append(int(user['default_profile']))\r\n",
    "    features.append(int(user['default_profile_image']))\r\n",
    "    urls = user['urls']\r\n",
    "    verified_urls, has_research_url = verify_urls(urls)\r\n",
    "    features = [*features, *verified_urls]\r\n",
    "    features.append(int(bool(user['location'])))\r\n",
    "    desc = user['description']\r\n",
    "    research_fields_count = get_description_research_fields_count(desc)\r\n",
    "    parsed_desc = parser.parse(desc)\r\n",
    "    mentions = parsed_desc.users\r\n",
    "    hashtags = parsed_desc.tags\r\n",
    "    urls = parsed_desc.urls\r\n",
    "    features.append(int(bool(desc)))\r\n",
    "    features.append(research_fields_count)\r\n",
    "    features.append(len(mentions))\r\n",
    "    features.append(len(hashtags))\r\n",
    "    features.append(len(urls))\r\n",
    "    features.append(int(research_fields_count > 0 or has_research_url))\r\n",
    "    return features"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# generate user features\r\n",
    "users = get_unique_users(path='tmp/dataset_demo_5000.csv')\r\n",
    "\r\n",
    "with open('tmp/users.json', 'r') as f:\r\n",
    "    user_data = json.load(f)\r\n",
    "\r\n",
    "with open('tmp/user_idx_demo_5000.json', 'r') as f:\r\n",
    "    idx = json.load(f)\r\n",
    "\r\n",
    "user_features = []\r\n",
    "for user_id in tqdm(users, total=len(users)):\r\n",
    "    user = user_data[str(user_id)]\r\n",
    "    features = generate_feature_vector(user)\r\n",
    "    features = [idx[str(user_id)], *features]\r\n",
    "    user_features.append(features)\r\n",
    "\r\n",
    "columns = [\r\n",
    "    'id',\r\n",
    "    'protected',\r\n",
    "    'verified',\r\n",
    "    'followers_count',\r\n",
    "    'friends_count',\r\n",
    "    'ratio_followers_friends',\r\n",
    "    'listed_count',\r\n",
    "    'statuses_count',\r\n",
    "    'default_profile',\r\n",
    "    'default_profile_image',\r\n",
    "    'has_url',\r\n",
    "    'url_is_google_research',\r\n",
    "    'url_is_linkedin',\r\n",
    "    'url_is_github',\r\n",
    "    'url_has_edu',\r\n",
    "    'url_is_academic',\r\n",
    "    'url_has_tilde',\r\n",
    "    'has_location',\r\n",
    "    'has_description',\r\n",
    "    'description_research_fields_count',\r\n",
    "    'description_mention_count',\r\n",
    "    'description_hashtag_count',\r\n",
    "    'description_url_count',\r\n",
    "    'researcher'\r\n",
    "]\r\n",
    "\r\n",
    "df = pd.DataFrame.from_records(user_features, columns=columns)\r\n",
    "\r\n",
    "df = std_norm(df, 'followers_count')\r\n",
    "df = std_norm(df, 'friends_count')\r\n",
    "df = std_norm(df, 'listed_count')\r\n",
    "df = std_norm(df, 'statuses_count')\r\n",
    "df = std_norm(df, 'ratio_followers_friends')\r\n",
    "df = std_norm(df, 'description_research_fields_count')\r\n",
    "df = std_norm(df, 'description_mention_count')\r\n",
    "df = std_norm(df, 'description_hashtag_count')\r\n",
    "df = std_norm(df, 'description_url_count')\r\n",
    "\r\n",
    "df.to_csv('data/features_demo_5000.csv', index=False)\r\n",
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# number of researchers found\r\n",
    "df = pd.read_csv('data/features_1.5.csv')\r\n",
    "count = len(df[df.researcher == 1])\r\n",
    "print(f'Found {count}/{len(df)}(~{int(count/len(df)*100)}%) researchers.')"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c0a21806fcacd0dbdaebc634b607eac64ce4f09eebc7449f37c85b0be514d7a5"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('tf': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}